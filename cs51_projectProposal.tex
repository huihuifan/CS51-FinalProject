\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\geometry{letterpaper}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage[superscript]{cite}
% http://arxmliv.kwarc.info/package_usage.php

\begin{document}

\title{CS 51. Project Proposal.}
\date{Due April 10th, 2015.}
\author{Angela Fan, Andre Nguyen, Vincent Nguyen, George Zeng.}
\maketitle

\section{Basics}
We plan to work on a machine learning project with the ultimate goal of implementing a neural network by hand.

Our project members are:
\begin{itemize}
  \item Angela Fan \href{mailto:huihuifan@college.harvard.edu}
    {\nolinkurl{<huihuifan@college.harvard.edu>}}
  \item Andre Nguyen \href{mailto:andrenguyen@college.harvard.edu}
    {\nolinkurl{<andrenguyen@college.harvard.edu>}}
  \item Vincent Nguyen \href{mailto:vnguyen01@college.harvard.edu}
    {\nolinkurl{<vnguyen01@college.harvard.edu>}}
  \item George Zeng \href{mailto:gzeng@college.harvard.edu}
    {\nolinkurl{<gzeng@college.harvard.edu>}}
\end{itemize}

\section{Brief Overview}
As a group, we are very interested in learning more about machine learning methods. For this project, we would like to explore neural networks, as they have seen increasing use, particularly in deep learning.

We will implement a basic version of a Restricted Boltzmann Machine (RBM), a type of stochastic neural network that has been used in a variety of applications. Specifically, we are fascinated by the potential to apply neural networks to everyday pattern recognition in image recognition and classification. However, we will treat the goal of image recognition as an extension to our core project, a little something extra should we have time to explore it. 

Since implementing a successful RBM is a nontrivial task that cannot be modularized easily compared to other potential projects, our main goal for this final project is to successfully implement a working RBM. Doing so will entails many subtasks that are very interconnected in nature and hence our subgoals will be very abstract rather than concrete; in other words, we are not working towards specific checkpoints of completed subtasks. 

After successful implementation of our RBM by hand, we will have the opportunity to pursue further side-projects that are more ``extension''-oriented in the sense that each application and consequent tweaking of our RBM implementation will provide a very concrete representation. Moreover, if we can apply our RBM to image/pattern recognition project, we will have concrete visualizations and other physical demonstrations. 

We plan on implementing our project in Python because it is a very popular for machine learning. Python's high-level nature will allow us to focus on building abstractions rather than get distracted by low-level computational concerns. The version of Python we will be using will be Python 3 (not Python 2).

\section{Feature List}

We believe that implementing an RBM will be a decent amount of work. We will implement the RBM as well as a few functions to help assess its performance. We will train and test our RBM on a toy dataset of 10 users and the movies they like. 

We will explore the following extensions, time permitting:
\begin{enumerate}
  \item Persistent CD algorithm to approximate sampling from the energy function probability distribution, as an alternative to CD-k
  \item Train and test our RBM on more complicated datasets, such as MNIST handwriting image recognition and Faces in the Wild, a set of labeled images of faces
\end{enumerate}

\section{Technical Specification}

A restricted Boltzmann machine is a simplified version of a Boltzmann machine in which nodes at hidden layers are restricted to non-cyclical graphs. The RBM will tune the parameters of an energy-based probabilistic function (linear in its parameters) so that desirable states have lower energy configurations. The energy function will be defined as 

\begin{center}
$E(v,h) = -b'v - c'h -h'Wv $ 
\end{center}

where $W$ represents the weights that connect the hidden and visible nodes, and $b$ and $c$ are offsets of the visible and hidden node layers. 

We will then sample from the probability distribution generated by the energy function using a Gibbs Sampler. In order to speed up the sampling, we will use the Contrastive Divergence algorithm with $k=1$ (referred to as CD-$k$). The CD-$k$ algorithm speeds up the sampling because it does not wait for the markov chain to converge, and allows us to initialize the markov chain with a known training example, avoiding the potential burn-in time. $\diamond \diamond \diamond$

We will implement this project in an entirely object oriented fashion. We will be coding a class RBM that takes three parameters:
\begin{itemize}
  \item The number of hidden states
  \item The number of visible states
  \item A learning rate parameter
\end{itemize}

In the init function, the RBM will initialize a set of all weights that will be 0. Subsequently, we will use three methods:
\begin{itemize}
  \item The first method will just train the neural network, which will allow the network to traing weights
  \item The second method will take the trained RBM, run it on the visible units, and will yield a generated sample of hidden units
  \item The third method will be a hidden method--it will take a trained RBM, run it on the hidden units, and then generate the visible units
\end{itemize}

In the training phase, we will implement the CD-$k$ algorithm with $k = 1$ and update the bias layer of the network. 

\section{Next Steps}

As an extension to our project, we intend to explore parameter optimization in the following ways:
\begin{itemize}
  \item Adjusting the learning rate parameter
  \item Adjusting the initial weights 
  \item Adjusting the initial biases
  \item Measuring and monitoring overfitting of the model
\end{itemize}

We will also explore the effects of using different types of units within the model. 


\end{document}
