{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import, print_function, unicode_literals, division\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random as rand\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Obtain complete MNIST images\n",
    "DATA_PATH = '~/data'\n",
    "mnist = fetch_mldata('MNIST original', data_home=DATA_PATH)\n",
    "\n",
    "subset = mnist.data[:100]\n",
    "\n",
    "#Convert to binary matrix \n",
    "for img in subset:\n",
    "    img[img < 100] = 0\n",
    "    img[img > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "class RBM:\n",
    "  \n",
    "  def __init__(self, num_visible, num_hidden, learning_rate = 0.1):\n",
    "    self.num_hidden = num_hidden\n",
    "    self.num_visible = num_visible\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "    # Initialize a weight matrix, of dimensions (num_visible x num_hidden), using\n",
    "    # a Gaussian distribution with mean 0 and standard deviation 0.1.\n",
    "    self.weights = 0.1 * np.random.randn(self.num_visible, self.num_hidden)    \n",
    "    # Insert weights for the bias units into the first row and first column.\n",
    "    self.weights = np.insert(self.weights, 0, 0, axis = 0)\n",
    "    self.weights = np.insert(self.weights, 0, 0, axis = 1)\n",
    "    print(self.weights.shape)\n",
    "\n",
    "  def train(self, data, max_epochs = 1000):\n",
    "    \"\"\"\n",
    "    Train the machine.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: A matrix where each row is a training example consisting of the states of visible units.    \n",
    "    \"\"\"\n",
    "\n",
    "    num_examples = data.shape[0]\n",
    "\n",
    "    # Insert bias units of 1 into the first column.\n",
    "    data = np.insert(data, 0, 1, axis = 1)\n",
    "\n",
    "    for epoch in range(max_epochs):      \n",
    "      # Clamp to the data and sample from the hidden units. \n",
    "      # (This is the \"positive CD phase\", aka the reality phase.)\n",
    "      pos_hidden_activations = np.dot(data, self.weights)      \n",
    "      pos_hidden_probs = self._logistic(pos_hidden_activations)\n",
    "      pos_hidden_states = pos_hidden_probs > np.random.rand(num_examples, self.num_hidden + 1)\n",
    "      # Note that we're using the activation *probabilities* of the hidden states, not the hidden states       \n",
    "      # themselves, when computing associations. We could also use the states; see section 3 of Hinton's \n",
    "      # \"A Practical Guide to Training Restricted Boltzmann Machines\" for more.\n",
    "      pos_associations = np.dot(data.T, pos_hidden_probs)\n",
    "\n",
    "      # Reconstruct the visible units and sample again from the hidden units.\n",
    "      # (This is the \"negative CD phase\", aka the daydreaming phase.)\n",
    "      neg_visible_activations = np.dot(pos_hidden_states, self.weights.T)\n",
    "      neg_visible_probs = self._logistic(neg_visible_activations)\n",
    "      neg_visible_probs[:,0] = 1 # Fix the bias unit.\n",
    "      neg_hidden_activations = np.dot(neg_visible_probs, self.weights)\n",
    "      neg_hidden_probs = self._logistic(neg_hidden_activations)\n",
    "      # Note, again, that we're using the activation *probabilities* when computing associations, not the states \n",
    "      # themselves.\n",
    "      neg_associations = np.dot(neg_visible_probs.T, neg_hidden_probs)\n",
    "\n",
    "      # Update weights.\n",
    "      self.weights += self.learning_rate * ((pos_associations - neg_associations) / num_examples)\n",
    "\n",
    "      error = np.sum((data - neg_visible_probs) ** 2)\n",
    "      print(\"Epoch %s: error is %s\" % (epoch, error))\n",
    "      if epoch == 10: \n",
    "            return neg_visible_probs\n",
    "\n",
    "  def run_visible(self, data):\n",
    "    \"\"\"\n",
    "    Assuming the RBM has been trained (so that weights for the network have been learned),\n",
    "    run the network on a set of visible units, to get a sample of the hidden units.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: A matrix where each row consists of the states of the visible units.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    hidden_states: A matrix where each row consists of the hidden units activated from the visible\n",
    "    units in the data matrix passed in.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_examples = data.shape[0]\n",
    "    \n",
    "    # Create a matrix, where each row is to be the hidden units (plus a bias unit)\n",
    "    # sampled from a training example.\n",
    "    hidden_states = np.ones((num_examples, self.num_hidden + 1))\n",
    "    \n",
    "    # Insert bias units of 1 into the first column of data.\n",
    "    data = np.insert(data, 0, 1, axis = 1)\n",
    "\n",
    "    # Calculate the activations of the hidden units.\n",
    "    hidden_activations = np.dot(data, self.weights)\n",
    "    # Calculate the probabilities of turning the hidden units on.\n",
    "    hidden_probs = self._logistic(hidden_activations)\n",
    "    # Turn the hidden units on with their specified probabilities.\n",
    "    hidden_states[:,:] = hidden_probs > np.random.rand(num_examples, self.num_hidden + 1)\n",
    "    # Always fix the bias unit to 1.\n",
    "    # hidden_states[:,0] = 1\n",
    "  \n",
    "    # Ignore the bias units.\n",
    "    hidden_states = hidden_states[:,1:]\n",
    "    return hidden_states\n",
    "    \n",
    "  # TODO: Remove the code duplication between this method and `run_visible`?\n",
    "  def run_hidden(self, data):\n",
    "    \"\"\"\n",
    "    Assuming the RBM has been trained (so that weights for the network have been learned),\n",
    "    run the network on a set of hidden units, to get a sample of the visible units.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: A matrix where each row consists of the states of the hidden units.\n",
    "    Returns\n",
    "    -------\n",
    "    visible_states: A matrix where each row consists of the visible units activated from the hidden\n",
    "    units in the data matrix passed in.\n",
    "    \"\"\"\n",
    "\n",
    "    num_examples = data.shape[0]\n",
    "\n",
    "    # Create a matrix, where each row is to be the visible units (plus a bias unit)\n",
    "    # sampled from a training example.\n",
    "    visible_states = np.ones((num_examples, self.num_visible + 1))\n",
    "\n",
    "    # Insert bias units of 1 into the first column of data.\n",
    "    data = np.insert(data, 0, 1, axis = 1)\n",
    "\n",
    "    # Calculate the activations of the visible units.\n",
    "    visible_activations = np.dot(data, self.weights.T)\n",
    "    # Calculate the probabilities of turning the visible units on.\n",
    "    visible_probs = self._logistic(visible_activations)\n",
    "    # Turn the visible units on with their specified probabilities.\n",
    "    visible_states[:,:] = visible_probs > np.random.rand(num_examples, self.num_visible + 1)\n",
    "    # Always fix the bias unit to 1.\n",
    "    # visible_states[:,0] = 1\n",
    "\n",
    "    # Ignore the bias units.\n",
    "    visible_states = visible_states[:,1:]\n",
    "    return visible_states\n",
    "    \n",
    "  def daydream(self, num_samples):\n",
    "    \"\"\"\n",
    "    Randomly initialize the visible units once, and start running alternating Gibbs sampling steps\n",
    "    (where each step consists of updating all the hidden units, and then updating all of the visible units),\n",
    "    taking a sample of the visible units at each step.\n",
    "    Note that we only initialize the network *once*, so these samples are correlated.\n",
    "    Returns\n",
    "    -------\n",
    "    samples: A matrix, where each row is a sample of the visible units produced while the network was\n",
    "    daydreaming.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a matrix, where each row is to be a sample of of the visible units \n",
    "    # (with an extra bias unit), initialized to all ones.\n",
    "    samples = np.ones((num_samples, self.num_visible + 1))\n",
    "\n",
    "    # Take the first sample from a uniform distribution.\n",
    "    samples[0,1:] = np.random.rand(self.num_visible)\n",
    "\n",
    "    # Start the alternating Gibbs sampling.\n",
    "    # Note that we keep the hidden units binary states, but leave the\n",
    "    # visible units as real probabilities. See section 3 of Hinton's\n",
    "    # \"A Practical Guide to Training Restricted Boltzmann Machines\"\n",
    "    # for more on why.\n",
    "    for i in range(1, num_samples):\n",
    "      visible = samples[i-1,:]\n",
    "\n",
    "      # Calculate the activations of the hidden units.\n",
    "      hidden_activations = np.dot(visible, self.weights)      \n",
    "      # Calculate the probabilities of turning the hidden units on.\n",
    "      hidden_probs = self._logistic(hidden_activations)\n",
    "      # Turn the hidden units on with their specified probabilities.\n",
    "      hidden_states = hidden_probs > np.random.rand(self.num_hidden + 1)\n",
    "      # Always fix the bias unit to 1.\n",
    "      hidden_states[0] = 1\n",
    "\n",
    "      # Recalculate the probabilities that the visible units are on.\n",
    "      visible_activations = np.dot(hidden_states, self.weights.T)\n",
    "      visible_probs = self._logistic(visible_activations)\n",
    "      visible_states = visible_probs > np.random.rand(self.num_visible + 1)\n",
    "      samples[i,:] = visible_states\n",
    "\n",
    "    # Ignore the bias units (the first column), since they're always set to 1.\n",
    "    return samples[:,1:]        \n",
    "      \n",
    "  def _logistic(self, x):\n",
    "    return 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '~/data'\n",
    "mnist = fetch_mldata('MNIST original', data_home=DATA_PATH)\n",
    "print(mnist.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "data_array = mnist.data\n",
    "print(data_array[0])\n",
    "print(data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnUtsa9t53/8fKb5JkZR09DpHNzeDDgPYKOqJW1hAjcBB\nASeZuLhFASMwigzaNPDITge5Ou2giQEHRjsI2vo6sNPAiVEjt86kzQ1Qou6gcRLYsdvYaQL4wtLR\nW4eiKL5Frg7Ete/i5n5J3Hxp/3/AhkiK5F7a1J/r8V/f94lSCoSQaBCbdwMIIbODgickQlDwhEQI\nCp6QCEHBExIhKHhCIsSjBS8inxCRH4nI34jI58JsFCFkOshjfHgRiQP4awAfB/AKwJ8BeEsp9UPj\nOTT4CZkjSimxP/bYHv4jAP5WKfW+UqoH4PcB/LzDCa3j7bffHrm/aAfbx/Y9pfa58VjBPwdwaNw/\nGj5GCFlgHit4DtcJWUJWHvm6VwD2jPt7uO/lRzg4OLBul0qlR55qNuzv78+7CZ6wfZPx1NtXqVRQ\nqVR8n/fYRbsV3C/a/UMAxwC+A4dFu8e8NyFkckQEymHR7lE9vFLqTkT+BYD/DiAO4B1T7ISQxeRR\nPXygN2YPT8jccOvhudOOkAhBwRMSISh4QiIEBU9IhKDgCYkQFDwhEYKCJyRCUPCERAgKnpAIQcET\nEiEoeEIiBAVPSISg4AmJEBQ8IRGCgickQlDwhEQICp6QCEHBExIhKHhCIsRj01QTMsZgMLAOpdTI\n/cFgMPH7x2IxxGIxiIh12zyIPxQ8CY27uzt0u130ej10u92xY1ISiQSSyeTIYT4mMpazkdig4Elo\n3N3dodPpoNlsjh2tVsuz5lkQMpkMstksstnsyO1YLIZEIkHBB4CCJ6Fxd3eHdruN29tb3Nzc4Obm\nBvV63bo9qeALhQIKhQJWV1exurqKwWAAEUEikUAmkwnpr3jaUPAkNEzBX19fo1qt4vXr19YxKcVi\nEWtra+h2u2NiZw2EYFDwJDRMwddqNVxeXuL8/Nw6JmV9fR3dbhf9fn9E7IVCIYTWRwMKnoSGvYe/\nvLzE6ekpjo+PcXJyMnEv3Gw2LbEnk0lkMhnk83n0ej328AGh4EloaME3Go0Rwb969Qo/+clPJhZl\nt9u1Fuh0z95ut3F3d0fBB4SCJ4Hx89lrtRpqtRqur69Hjmq1imq1GsoqvX3hzrydSqVcPXr69PdQ\n8CQw2md389qPj49xenqKy8tLXF9f4/b21uqBw6Df76PT6aDRaKBWq+Hq6grJZBLxeBxKKWQyGVeP\nnj79PRQ8CYwesmtf3e6zn52d4eLiYmqCt68RJJNJxGIxKKXQ6/WQz+dH/Hl96AU+Cp6CJw/AnKNr\nb93026+uriwrbhY9vO7Ze70eWq2WNbTXh7mazzn+PRMJXkTeB3ADoA+gp5T6SBiNIouHUmqshzU9\n9mq1ilqtNrLRxhR8GIIzz6/F3u120Ww2cXNzg7W1NZTLZXQ6HfT7fQDgphwbk/bwCsC+UmryXRVk\n4XGy3bTHfnFx4bqlttfrhXJ+s4c3xV6v11GtVtFoNNDpdMY25dC2+4AwhvScGEWEXq/n6LOfnJzg\n5OQEnU5nZBFP3w97Dm8O4+v1urUo1263HTflUPAfEEYP/yci0gfwH5RS/ymENpEFxW1jzdHREQ4P\nD9Hv90dsOvN+GOgevtfrjVlu8Xgc3W53bFNOq9WiT28wqeA/qpQ6EZFnAN4TkR8ppb6tf3lwcGA9\ncX9/H/v7+xOejkwTu69ueu39ft/RYw/TZw/aPjey2Szq9ToajQba7bb15aDn80+ZSqWCSqXi+zwJ\n60MSkbcB3Cqlvji8r/itulzYfXXTa+90Ojg+Pra2yerb5mPz/ryfP3+ON9980zreeOONkfvJZHKu\n7ZslIgKl1Nh0+9E9vIhkAcSVUnURyQH4WQAvJ2gjmSPmKry54GYuwOkFumn57GT6TDKk3wLwh8PN\nDCsAfk8p9cehtIrMBXNRzime3bThKPjl5NGCV0r9GMCHQmwLmTN3d3fWyrf22c2Ydvtmm7B9djJ9\nuNOOWDitwl9cXFhDeSefnT38ckHBEwDOO+ns8ex68a7X64147hT88kDBEwu3jTXaZ3fz2MPy2cn0\noeAjhJfPfnd35xnLXq1Wp94+v7zzXrH45t/R7XbRbrfRarXQaDRQr9dRq9V8bTmvcz+VeHoKPiLo\nIbub16599rOzs7nZbvF4fCzvvBnX7tV+nevOjKbLZrNIpVJWoE0ikfA8vz6PU/77pxJPT8FHCL3/\n3CmWvdFoWItzV1dXcxH8ysoK0un0WN55fd/MeW/+DYPBwNpRZwreTI7R6/WwsuL97+527qcUT0/B\nRwi9KKe9ddNjt/vstVrNij6bpeBTqRRyudxIXLtOY6WH57q9ZvILvWder0HYk2O0Wi3E43HP8+vz\n6vM9xXh6Cj4iOK3Cmx57tVq1hKTj2nUPP6u96PF4HOl0Gvl8HqVSCeVy2YpxX1tbQ71ex+vXrx3F\nLCIjPbw9OUa9XvcVfKFQwNramhVP/xTz3lPwEUL/8z8knn0eQ/pcLodisYiNjQ1sbm7i2bNn2Nzc\nRLVaHRumazED4/Hy5u+r1arvwlu5XLbWAgA8yXh6Cj5CmEP6arXq6LM7HbNctEulUlYPv76+jq2t\nLezs7GB3dxf5fH4k043+8tLza/33DQYDKzlGMplEKpUKtOh2e3triX1lZeVJxtNT8BFB93jmkP7i\n4sLy2Y+Ojlw99ln57E49/Pb2Nl68eIG9vT1kMhlrgU733Ol0GisrKyNDev17e7y8H3pqYIpdZ+yh\n4MnMcfOetS/thVs8+yx9dj9isZi1cKdXyvP5PFZXV61cdYVCAdlsFul0GqlUColEwhLzpF9O6XTa\niqdvtVoj8fQUPJkpuod2ywnvlzduMBhYPvvV1RVqtRqj3SIIBb8kOMWrm550q9XyfP1gMBiLZ9e2\nWxQywpB7KPglwpy7mn60vu+FUmrMZ2cPHz0o+CXBvuhm99BrtZrn6weDgWc8O4kGFPwSYSao0Laa\n9tCvrq48X6uUsqYAjUbDmgrMcicdmT8U/JJgbiQxbTWdE/78/Nz39W4+O+fw0YGCXxLMRTv7xpmj\noyMcHx/7voebx8549uhAwS8R/X7fyjajizrqWO9F8NGnjfbpk8kk0uk0MpkMcrkcCoUCisUiOp2O\nZ6w8oeDJEqG33uqdeOvr69aURETQarVcY+W73e6T2TwzCRQ8WRr01lu9177b7Y4Ujry9vXWM9dfr\nHxQ8BU+WCHsPbw9htduOZqDNU0heEQYUPFkazB5eiz2ZTCKbzWJ1ddXak6DTWgEfbFYi91DwZGkw\ne3hT7IVCAeVyGblcDqlUaiw5xlNJTxUGFDxZGnQPb6//rhfl0um0Y3IMCv4DKHiyNOgePpFIjFlu\n/X4fyWRyLDlGJpOx4uUJBb9QeMW7dzqdkVjtdrttZaMJy2O252G352h/SPud/HC/vPOFQgG5XA6Z\nTMbKUrOysmKd268dzWbTeg+neHlCwS8MZry7k4fcarXG4tnDDm/Vm1rs+eD14YeXB97tdl3zzutz\nbW9vY3t7GxsbGyiVSsjn88hkMhyShwgFvyCY0XBOySTteePDTiOtUzvp3OxO+dn9sPvfZt74brc7\n9v72c+hklc+ePUO5XEahULBSWJFw4JVcEPReeb3Q5FWf3V4oIqweXi+E6bRSZo721dVV39fb26zL\nQ3W7XQCjtpr5vvo8a2tr1qF7+HQ6zR4+RHwFLyJfAfCPAJwrpX5m+NgagD8A8FMA3gfwKaXU9RTb\n+eRxSqvsljdeH2EXitCCLBQKKJVKIznhy+Wy7+t1e7U1ZmaPNUcQuVzOen99lMvlkS+A1dVVa0jP\nHj48glzJ3wHw7wF8zXjs8wDeU0p9QUQ+N7z/+Sm0LzK4hb+aw3inoX7Yc3hz6+rGxsbIMNuPXC5n\n5Y03xa4Fa09DbX//XC6HXC43MuRnDx8uvoJXSn1bRN60PfxJAB8b3v4qgAoo+Ikw5/C6h9dppI+P\nj3F+fj6yABZ2fXazBzYFub29jd3dXezs7Pi+h1kRRov95ubGKuJovr8OfjHfP51OOy7osYcPj8de\nyS2l1Nnw9hmArZDaE1nsc3gd735ycoLDw0Ocnp66xrKHZcslEglHweu88H6YO9y02M1FN/uQXr//\n8+fPsbe3h0QiMWbVxeNxy8ojkzPxV6dSSomIYxjSwcGBdXt/fx/7+/uTnm6p8fLZW62WZ874MOLd\nvXz2lZUVFItFlEqlkaNcLluHH7oenb1I5c3NDYrFonW4nWPefrmuL2/mHGg2m7i9vcXNzQ263a5r\n7XgRmeuXUqVSQaVS8X3eYwV/JiLbSqlTEdkB4JhfyRR81LH77Ha/utFo4OTkZGo+ux6yu9VeT6VS\n2N3dHfPBH2KL2acEzWbTCmGNxWLY2tpyff9F6MHt5aavrq5G6svncjnH2vH6sXn+DfYO9eXLl47P\ne6zgvwXg0wB+c/jz3Ue+T2QwF+XsPnWz2US9XrcW6aa1sUbbbk6113O5HDY3N7G5uYn19fVQBG+v\nwqqLQ2rBFwoFa2PNImAXvCn2Xq9ntde8dnp/wqL8DX4EseW+jvsFug0ROQTw6wB+A8A3ROQzGNpy\n02zkU8C+Cm8f8uo0VU712cNehdceuN0LN22yYrFoRZ89RPD2ePVYLGZFtWmLT9twD/1CmTZ2wdsD\ncYrF4sg1MwtPLktyjSCr9G+5/OrjIbflSePks5uFIarV6tjcd1o76ez11+0+uP7H1oIMOrc2318P\n4814dfsXzCIP6Z2i7vRagzly0YUnn4zgSTi42W56GH95eTkyzNe546e1k06vkuth/LNnz8aGqtoH\nf+iQ3knszWbTcTi8qD280xe0ubPR/AJdpvRZi3GlI4DTP5DOK699dr06PK288eaQvlwuj/nsboEt\nDxW8KXZzz4BX4MwioAVv/6x0O5vNJu7u7qw1CX0tKXgyhpPg7T67m20Xhs9u/pPqHv7Zs2eWz/7i\nxYsx7/shobEArFDWZDLpmAPfzWNfFJ/dq758LBZDu90GMDpSWrb68RR8SCilPHOit9vtsfrspsd+\nfe0fiuDlAfuJMh6PT+yzB23fsuL35ZpOp1EqlZa6fjwFHxJ6O6mb195oNCaKZ9c9tFOcepBhcSwW\nw+7uLra2trC+vo5isbhwq+Rk+vCTDgm7z26PC6/X667x7EEF7xRPrn30TCbj+fpYLGYt0GkfXNtu\n897hRmYHBR8S5h5yp3j2Wq02Vp/9oT67mbjRtLj0fS9EZMxnZw8fPfhJh8Qk8ewPGdLrRTe7h14s\nFj1fH4vFHOPNKfhowU86JNxstzDj2fUmD9NW0x76+vq652tFxJoC6JhznSySgo8O/KRDws9nv7i4\nmKg+u5mL3bTVdnZ2sLOzg83NTd/Xu/ngnMNHBwo+JOyC1xlrtM9+dnY2kc9uLtrZN868ePECu7u7\nvu/h5rEvs5VGHgYFHxJKqZH67a1Wa6R++6Q+eyqV8vTQw/DRydOHgl8QTJ/dKS98JpMZ89Fpq5GH\nQsEvCOYc3SnIxB6v/pjwVUL4n7Ig2Ofo9pztZry6PUEFe3gSFAp+QbBXRNXzcq+87ezhyUPhf8qC\n4Ga7mcN4p3h1zuHJQ6DgFwR7jHW5XLbCV3d3d7G5uTmyoKerqzJvO3kI/E9ZEMx0SabPvrOzg729\nPWxvb7t66PTRSVAo+ID4xbvr5JQ6LdVjYqXj8TgSiQRSqZRVsKFQKKBYLNJnJ6FAwQdEx7u7xbxP\nGu9OyCyg4AMSJK/8JPHuhMwCCj4g9r3y9pTSZrx7tVql4MlCQsEHZDAYeOaVv76+toSvvwwoeLJo\nUPAB0XN4t3j3169fT71+OyGTQsEHxJ7Cyh7vfnl5OVG8OyGzgIIPiDmHv7m5GRH84eEhzs/Pp5pX\nnpAwoOADEka8OyHzhlu0CIkQFDwhEYKCJyRC+ApeRL4iImci8gPjsQMRORKR7w6PT0y3mYSQMAjS\nw/8OALugFYDfUkp9eHj8t/CbRggJG1/BK6W+DaDq8Kv51/clhDyISebwvyIifyki74hIKbQWEUKm\nxmN9+N8G8K+Ht/8NgC8C+Iz9SQcHB9bt/f197O/vP/J0TwOvvPM68UUul0Mmk0E6nbay2TDBxT16\nE5NbXoJ6vT5Su73b7eLu7i7wxicR8fyMCoUC8vk8stks0uk0UqkUEokE4vE4ROY74K1UKqhUKr7P\ne5TglVLn+raIfBnAHzk9zxR81HHKO2/mns/lctjZ2WHeeQ/6/f7YtmUzN8HZ2RlOTk5wcXGBarWK\ner2OVquFXq8XKAnJysqK42ekPydd2uvZs2col8soFArIZDJIJBJzF7y9Q3358qXj8x4leBHZUUqd\nDO/+IoAfeD2fjCap1IUczWSUhULBSlpJwTtzd3eHTqczEpxk5iW4vLzE+fk5Li8vcX19/WDBx+Nx\npNPpsc9G319fX7eKdy6a4IPiK3gR+TqAjwHYEJFDAG8D2BeRD+F+tf7HAH55qq18Atiz0tpTTus0\nVmb9dgp+lH6/j3a7jdvbW9zc3IzkJKjX6yPhytVqFbe3t2i32w8SfCqVQi6XG6sJsLq6OpY2/EkK\nXin1lsPDX5lCW540Tnnn9T+P+Q9kzzvP+u0foHv4RqNhJRwx8xLYE5LoHv7u7i7Q+5uCLxaL1udi\nfgnbC4Q8OcGTcHBLQ62H8RsbGyNDSV3DnT38B5g9fK1Ww9XV1UhOAp1E1D7kf0wPXywWsb6+PlIb\noFAojNUFoOCJI049vF4E0nnndcZa1m93xuzhr6+vcXl5idPTUysnQavVQrfbtVbozYW9xwh+bW0N\nW1tb2N3dxe7uLnK5nOuCHgVPRnASvD3vvJslRFvuHqce/uzsDEdHRzg8PESn03HNR/DYHn5rawvP\nnz/HG2+8gUwm42nbLQMU/IwQEc+886US9y75+ey1Ws06rq+vR45qtYper+f5/lqYbqItFosjR6lU\nGjnS6fSMrsT0oODJwuDks5teux6+P9Znj8fjI3sf7Mci++xhQcGTheHu7g7tdtsx73+r1bIW6LTP\n/ljbzWkfxFPx2f2g4MnCYC7KmRab9ttNC87s4R9ju9ktUNNn16W6KXhCpoi5KKfn5abXXqvVRr4I\nbm9vJ7LdzH0Qa2trY18AFDwhU0QP6fXGGr1VVg/lG43GmM/ebrcD9/ArKyvWYqlehdce++bmphUY\ns8w+ux8UPFkY+v0+Op2O1cNr283uszst6k1iu2mfPZvNLr3P7gcFTxYGpx7+9PTU8tm73a6jXRe0\nJLcWfD6fR6lUGvPZ0+n00vvsflDwZGYE9dntHruez/sN3Sf12VOp1IyuxPyg4MnMcPPZ9bD89PQU\np6enI7bbY3x2N699c3PzyfvsflDwZGaYPrtTPLtenDM31jxkUc702Z289o2NjSfvs/tBwZOZoQV/\ne3s7EsvuFM/+mI01KysrTz6efVIoeDIz7NFu9oQV5heA6bM/pIdPp9PI5/MjPrsW+lOIZ58UCp7M\nDPvGmsvLy5FhfKPRGPPaH5uxRq/C6yG8PZ49k8lYCUMpeEKmgDmk1+Gtp6enOD4+xsnJCdrttuOC\n3iQba7TPvrOzEwmf3Q8KnswM02c3E1i8evUKh4eH6PV6jh77Q+PZ9ZDe9Nn39vYi4bP7QcGTwNgT\nStjF6YdXLHu1WkW/3/d8vSlOP5/d7rGXy2Ukk8mwLsXSQsGTwNzd3Tnmg9eHH04+u7bdHuqzO/nt\nW1tbkffZ/aDgSWD84tX9RGvPG29urAmCtt2ccsabPvvGxgZKpRIF7wAFTwJjzsHtFlq9XvcVvA6I\nMePZH7qxRttudo+9UCiMpf3O5/MUvA0KngRCKTWyyu7ko/vh5LM/ZEhvbqxxyhtvL+xRKBSY198G\nrwQJjF3weoiufXQ/7LHsD41nt2+sseeNj0I8+6RQ8CQwvV5vTPA6seTJyYlvL+0VOBMEs4cvlUrY\n2NjA9va2lds/k8kgmUyO5fan4D+AgieBcerhzXh1P8G7eewP8dntPfzW1hZevHiBvb09pFIpR7tu\nEco5LwoU/AwZDAaWtdXpdNBqtdBoNFCv15HNZj0LUYTxD+tUnCGoj66U8oxVr1argUTrxaQ+eyKR\nmOj8UYCCnxFKKfR6PbRaLdTrdVSr1ZH5Zbvddt32qX9Oen7TR3fy0/1ef3x87Oqjh4GTz25eAz18\np8/+eCj4GeEkeP2P2u/30Ww2HRecstmsVbFmUvT5nTz0ZrPp+3q9QDctweu98E6x7JlMZqTwZqlU\nQj6fRzqdpuAfAAU/I+yCN8WuEzeatpLO36b3h4eBnoPrWHR7fXU/3OLVwxS8PZ7d9Nvd4tlpuwXH\n80qJyB6ArwHYBKAA/Eel1L8TkTUAfwDgpwC8D+BTSqnrKbd1qbELXou92+2i2Wzi5uYGa2trVmZW\nU+x+e8yDnt++6GYWdri+9v74lFIT++h+mItyel7uFc/OjTUPx++rsQfgs0qp74lIHsBfiMh7AH4J\nwHtKqS+IyOcAfH54EBdMwYsIBoOBJfZarYZ8Pu8o9lwuF1oPqs/v5KNfXl76tn9SH90Pe3irmZJq\nc3MTuVxubNrDjTUPw/NKKaVOAZwOb9+KyA8BPAfwSQAfGz7tqwAqoOA90YI3xd5oNKyFqWw26yj2\ndrsdSg8PjA7pq9WqZavpxTg/tLvQ6/VGarCHuWjnlEba7rNHOZ59UgJ/NYrImwA+DOBPAWwppc6G\nvzoDsBV6y54YWvB6zm63nNLp9JjYV1dX0el0QhvS2zfOXFxcWD760dGR73u4eehBQmOD4NTDb29v\nWz57Mpmkzz4hgQQ/HM5/E8CvKqXq5sVVSikRcZzAHRwcWLf39/exv78/SVuXGqUU+v2+q3jb7bZj\ngUMdQ57JZCY6f7/f94xFD7IXflKC+ux2j13P5zl0d6dSqaBSqfg+z/cKikgC92L/XaXUu8OHz0Rk\nWyl1KiI7AM6dXmsKnngTxKefhMFggOPjY5ydneHq6gq1Wi30VXY/3Hx2PSzf3t7G9vb2iO3GRblg\n2DvUly9fOj7Pb5VeALwD4K+UUl8yfvUtAJ8G8JvDn+86vJw8AD+fvtFoTPT+g8FgzEdvNBqhTRmC\nYPrsTvHsenHO3FjDRblw8buSHwXwTwF8X0S+O3zs1wD8BoBviMhnMLTlptbCiGCfY+v5ql7gu7m5\nmfj9TR99Hj28Fnw+nx+bvtjj2bmxZjr4rdL/LwAxl19/PPzmRBftk9t9ep3HPZ/PT/T+g8HA00ef\nBeaiXKlUGktYYV+/0EN69vDhwSu5INh9elPsYSzamT66zv3earXQ6XRmOoc3bbeNjY2RYXwulxvz\n2tnDhwsFvyCYgjd34JmLWpO+v1s8+qzn8GZ46/b2tpU3Pp1OOy7osYcPD17JBUEP6c0deHbbalKc\n/PMwfXQ/7EN67bPrvPGJRMLRY49S3vhpQ8EvEF4+/SJgj89/6BeSVyx7uVxGPB6fwV8RbSh4EpiV\nlRXP+ut+OPns2nZjDz4bKHgSGL94dT/ReuWNJ7OBgieBMefgdgutUCj4Ct4ez2728GQ28EqTQIjI\nyCq7k4/uh5PPziH9bKHgSWDsgtdDdO2j+2GPZWc8++zhlSaBSSQSY4LXiSV3dnZ8e2mvwBkyGyh4\nEhinHt6MV/cTvJvHTp99dlDwARERK3tsKpWyVqt1YUNdPdVpU0vQQgvTxinXfVAfXUQ8Y9XL5TJF\nuwRQ8AERESQSCWQyGRQKBZTLZavUsVIK6XTas3b6rPare7Xf9NGd/HS/1+/u7rr66GQ54CcVELvg\n2+22JfZ4PI5sNjuW573ZbFqBMPMWPACr/U4eejab9X29XqCj4JcXflIBsQtep2Y2I8DMHO/2vPOL\ngJ6D61h0e311P9zi1Sn45YGfVEBisRiSyaQleAAjCScLhYKVliqZTI6IfRH2iDv56OZGmFKp5Pt6\n+ujLDwUfEN3D66GvWSVFB4XobaKxWGwknn0RBA98MKR38tE3NjY8Xysi9NGfAPykAmIO6c2eXS/K\n3d7ejg3jG40GUqnUwgjeHNKXy2XLVtOLcX7o2uvaqWC8+vLBTyogWvBa7HbbzVygMyvKLIrgdfvN\nIf2zZ88sH/3Fixe+7+HmoYcRq09mAwUfEO3Du4k3lUqhXq9bx83NzUg983kv3MXjcc9Y9CB74cny\nQ8GHhJ9PH8T2miaxWAy7u7vY2trC+vo6isUiV9kjCD/pkHASvOnTB7G9pkksFhvz0XO53MJMOchs\noOBDwi54U+ypVMrX9ppF+0wfnT18NOEnHRJ2wZtiz+VyE1eOmZRYLObpo5NowE86JEzB28VeLBYn\nrg0XRvu0d65zv2cyGaRSKQo+QvCTDgk/n77X6829fW7x6JzDRweZVtimiKhFCAmdFV6hsbPM/e6F\nk39OL/1pIiJQSo3td6bgCXmCuAmeX+uERAgKnpAI4Sl4EdkTkf8hIv9XRP6PiPzL4eMHInIkIt8d\nHp+YTXMJIZPgOYcXkW0A20qp74lIHsBfAPgFAJ8CUFdK/ZbHazmHJ2ROuM3hPW05pdQpgNPh7VsR\n+SGA5/o9Q28lIWSqBJ7Di8ibAD4M4H8PH/oVEflLEXlHROa7b5QQEohAgh8O5/8LgF9VSt0C+G0A\nPw3gQwBOAHxxai0khISG7047EUkA+CaA/6yUehcAlFLnxu+/DOCPnF57cHBg3d7f38f+/v5krSWE\nOFKpVFCpVHyf57doJwC+CuBKKfVZ4/EdpdTJ8PZnAfw9pdQ/sb2Wi3aEzIlH7bQTkb8P4H8C+D4A\n/cR/BeAt3A/nFYAfA/hlpdSZ7bUUPCFzgltrCYkQ3FpLCKHgCYkSFDwhEYKCJyRCUPCERAgKnpAI\nQcETEiEoeEIiBAVPSISg4AmJEBQ8IRGCgickQsxM8EFidecJ2zcZbN9kzKp9FPwQtm8y2L7JeHKC\nJ4TMHwqekAgx1QQYU3ljQkggZprxhhCyeHBIT0iEoOAJiRAzEbyIfEJEfiQifyMin5vFOR+CiLwv\nIt8fFsap9gmCAAACVElEQVT8zgK05ysiciYiPzAeWxOR90Tk/4nIH8+z2o9L+xaiwKhHAdSFuH7z\nLtA69Tm8iMQB/DWAjwN4BeDPALyllPrhVE/8AETkxwD+rlLq9bzbAgAi8g8A3AL4mlLqZ4aPfQHA\npVLqC8MvzbJS6vML1L634VNgdEZtcyuA+ktYgOs3SYHWMJhFD/8RAH+rlHpfKdUD8PsAfn4G530o\nC1McUyn1bQBV28OfxH1REAx//sJMG2Xg0j5gAa6hUupUKfW94e1bALoA6kJcP4/2ATO4frMQ/HMA\nh8b9I3zwBy4KCsCfiMifi8g/m3djXNgyin2cAdiaZ2NcWKgCo0YB1D/FAl6/eRRonYXgl8H3+6hS\n6sMAfg7APx8OWReWYYWPRbuuC1VgdDhc/ibuC6DWzd8twvWbV4HWWQj+FYA94/4e7nv5hUHXyVNK\nXQD4Q9xPQxaNs+H8DyKyA+Dc5/kzRSl1roYA+DLmeA2NAqi/qwugYoGun1uB1llcv1kI/s8B/B0R\neVNEkgD+MYBvzeC8gRCRrIgUhrdzAH4WwA+8XzUXvgXg08PbnwbwrsdzZ85QRJpfxJyu4bAA6jsA\n/kop9SXjVwtx/dzaN6vrN5OddiLycwC+BCAO4B2l1L+d+kkDIiI/jfteHbgvn/17826fiHwdwMcA\nbOB+vvnrAP4rgG8AeAPA+wA+pZS6XpD2vQ1gHz4FRmfUNqcCqL8G4DtYgOs3SYHWUM7PrbWERAfu\ntCMkQlDwhEQICp6QCEHBExIhKHhCIgQFT0iEoOAJiRAUPCER4v8DPueW02u8QXAAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1090c0c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert image to binary matrix\n",
    "for img in data_array:\n",
    "    img[img < 100] = 0\n",
    "    img[img > 0] = 1\n",
    "\n",
    "plt.imshow(data_array[0].reshape(28, 28), cmap = 'Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  r = RBM(num_visible = 784, num_hidden = 2)\n",
    "  d = r.train(data_array, max_epochs = 11)\n",
    "  print(r.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-37a5a976ed63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Greys'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "plt.imshow(d[0].reshape(28, 28), cmap = 'Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
